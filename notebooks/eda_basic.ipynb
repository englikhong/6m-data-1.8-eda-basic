{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZw5DITs6Nw8"
      },
      "source": [
        "# EDA Basic Lesson Plan\n",
        "\n",
        "Welcome to the Exploratory Data Analysis (EDA) lesson! This notebook is designed to guide you through the essential steps of taking a raw dataset, understanding its structure, cleaning it, and transforming it for analysis.\n",
        "\n",
        "**Structure:**\n",
        "* **Section 1: The First Look:** How to read files and get a quick statistical summary.\n",
        "* **Section 2: The Cleanup:** Handling the messy reality of data (missing values, duplicates, outliers).\n",
        "* **Section 3: Refinement & Transformation:** preparing data for machine learning or deep analysis (mapping, string manipulation).\n",
        "\n",
        "**For Instructors:** Use the narrative to introduce concepts before running code cells.\n",
        "**For Learners:** Read the comments in the code to understand exactly what each parameter does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv7hzUvQ6Nw9"
      },
      "source": [
        "### Setup\n",
        "As always, we start by importing the necessary libraries. `pandas` is our primary tool for data manipulation, and `numpy` helps with numerical operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SKrab-_h6Nw-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Y3hKTw6Nw_"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA_RYyq46NxA"
      },
      "source": [
        "# Section 1: The First Look: Inspection & Summary\n",
        "\n",
        "**Goal:** Load data from different file formats and get a \"health check\" of the data using descriptive statistics.\n",
        "\n",
        "We will cover:\n",
        "1. Reading CSV and Text files (handling headers, delimiters, and special characters).\n",
        "2. Summarizing data using `.describe()`, `.sum()`, and `.value_counts()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvvNx0Kv6NxA"
      },
      "source": [
        "## Part 1: Reading Data\n",
        "\n",
        "Pandas is incredibly flexible with input formats. Let's start by reading a standard Comma-Separated Values (CSV) file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVEbfk406NxB",
        "outputId": "e174c436-cdf3-45f0-c189-73054c97f8d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: 'https://github.com/englikhong/6m-data-1.8-eda-basic/blob/main/data/ex1.csv': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Let's inspect the raw file content first using a shell command\n",
        "!cat ../data/ex1.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "zyDfRTYw6NxB",
        "outputId": "be39b4f3-05d8-45b5-e238-dc98a163feff"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../data/ex1.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2776548097.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use read_csv to load the data into a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# By default, it assumes the first row is the header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/ex1.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/ex1.csv'"
          ]
        }
      ],
      "source": [
        "# Use read_csv to load the data into a DataFrame\n",
        "# By default, it assumes the first row is the header\n",
        "df = pd.read_csv(\"../data/ex1.csv\")\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLGPk6Bt6NxC"
      },
      "source": [
        "**Scenario:** What if the file doesn't have a header row? If we don't specify this, pandas will mistakenly use the first row of data as column names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km_jZL4D6NxC"
      },
      "outputs": [],
      "source": [
        "!cat ../data/ex2.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRxxdl376NxD"
      },
      "outputs": [],
      "source": [
        "# Option 1: Tell pandas there is no header. It will assign integers (0, 1, 2...) as column names.\n",
        "pd.read_csv(\"../data/ex2.csv\", header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW8ukXyL6NxD"
      },
      "outputs": [],
      "source": [
        "# Option 2: Provide your own column names using the 'names' parameter\n",
        "pd.read_csv(\"../data/ex2.csv\", names=[\"a\", \"b\", \"c\", \"d\", \"message\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Zp8QNi6NxD"
      },
      "source": [
        "**Indexing:** You can also designate a specific column to be the index (row labels) of the DataFrame, rather than the default 0, 1, 2... index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cntMh_1y6NxE"
      },
      "outputs": [],
      "source": [
        "names = [\"a\", \"b\", \"c\", \"d\", \"message\"]\n",
        "\n",
        "# Use 'index_col' to set the 'message' column as the index\n",
        "pd.read_csv(\"../data/ex2.csv\", names=names, index_col=\"message\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ2XbEwO6NxE"
      },
      "source": [
        "**Irregular Separators:** Sometimes data isn't separated by commas. It might be tabs, spaces, or a variable amount of whitespace. We can use Regular Expressions (Regex) to handle this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y64JyzOr6NxE"
      },
      "outputs": [],
      "source": [
        "# Inspect a file with messy whitespace\n",
        "!cat ../data/ex3.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6S2JxYQ6NxE"
      },
      "outputs": [],
      "source": [
        "# sep=\"\\s+\" is a regex that means \"one or more whitespace characters\"\n",
        "result = pd.read_csv(\"../data/ex3.txt\", sep=\"\\s+\")\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2wwpg4v6NxE"
      },
      "source": [
        "**Skipping Rows:** Sometimes files contain comments or metadata at the top that we want to ignore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmtN3NyF6NxE"
      },
      "outputs": [],
      "source": [
        "!cat ../data/ex4.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gtnsw3on6NxF"
      },
      "outputs": [],
      "source": [
        "# Skip specific rows by index (0, 2, and 3 here) to get to the clean data\n",
        "pd.read_csv(\"../data/ex4.csv\", skiprows=[0, 2, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjbqOfjT6NxF"
      },
      "source": [
        "**Handling Missing Values (at load time):** Pandas is smart about identifying missing data (empty strings, 'NA', 'NULL'), but we can also define our own \"sentinels\" for missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtHZg7_K6NxF"
      },
      "outputs": [],
      "source": [
        "!cat ../data/ex5.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIs_42Da6NxF"
      },
      "outputs": [],
      "source": [
        "# Default behavior: reads 'NA' and 'NULL' as NaN (Not a Number/Missing)\n",
        "result = pd.read_csv(\"../data/ex5.csv\")\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrSsvGQM6NxF"
      },
      "outputs": [],
      "source": [
        "# Verify where the missing values are detected\n",
        "result.isna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPvfVyzN6NxF"
      },
      "source": [
        "You can customize what is considered \"Missing\" for each column specifically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-E58d5rP6NxF"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary of sentinels\n",
        "# For column 'message', treat 'foo' and 'NA' as null\n",
        "# For column 'something', treat 'two' as null\n",
        "sentinels = {\"message\": [\"NULL\", \"NA\"], \"something\": [\"two\"]}\n",
        "\n",
        "pd.read_csv(\"../data/ex5.csv\", na_values=sentinels, keep_default_na=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbWw8rdp6NxF"
      },
      "source": [
        "**Reading Excel:** Reading Excel files works similarly, but we can specify sheet names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpWZhoSY6NxF"
      },
      "outputs": [],
      "source": [
        "# Load the Excel file object\n",
        "xlsx = pd.ExcelFile(\"../data/Resaleflatpricesbasedonregistrationdate.xlsx\")\n",
        "\n",
        "# List available sheets\n",
        "xlsx.sheet_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaTWTTzO6NxG"
      },
      "outputs": [],
      "source": [
        "# Parse a specific sheet into a DataFrame\n",
        "xlsx.parse(sheet_name=\"2017\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMzVmYNS6NxG"
      },
      "outputs": [],
      "source": [
        "# Shortcut: Read directly without creating an ExcelFile object first\n",
        "frame = pd.read_excel(\"../data/Resaleflatpricesbasedonregistrationdate.xlsx\", sheet_name=\"2017\")\n",
        "\n",
        "frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr92WbOE6NxG"
      },
      "source": [
        "## Part 2: Summarizing and Computing Descriptive Statistics\n",
        "\n",
        "Once data is loaded, we need to understand its \"shape\". We use summary statistics to get a birds-eye view."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLH3ubae6NxG"
      },
      "source": [
        "> **ðŸ“Š Visual Illustration Available**: See ![descriptive_statistics](https://github.com/englikhong/6m-data-1.8-eda-basic/blob/main/visualisations/01_descriptive_statistics.png?raw=1) for visual representations of descriptive statistics including histograms, box plots, and how statistics like mean, median, and quartiles relate to data distributions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA8xilcY6NxG"
      },
      "source": [
        "Let's create a sample DataFrame with some missing values to test our summary methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lv0KnYxv6NxG"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame([[1.4, np.nan], [7.1, -4.5], [np.nan, np.nan], [0.75, -1.3]],\n",
        "                  index=[\"a\", \"b\", \"c\", \"d\"], columns=[\"one\", \"two\"])\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxAcc6Wl6NxG"
      },
      "source": [
        "**Reductions:** Methods like `.sum()` or `.mean()` reduce a Series of numbers to a single value. By default, they operate down the rows (`axis=0`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owEy24x46NxH"
      },
      "outputs": [],
      "source": [
        "# Sums down the rows (returns sum for each column)\n",
        "df.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8ADdPXP6NxH"
      },
      "outputs": [],
      "source": [
        "# Sums across the columns (returns sum for each row)\n",
        "# axis=1 is synonymous with axis='columns'\n",
        "df.sum(axis=\"columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t2lhJwR6NxH"
      },
      "source": [
        "**Handling NAs in Calculations:** By default, pandas ignores NaNs (treats them as zero for sums). We can change this with `skipna`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYbylqEm6NxH"
      },
      "outputs": [],
      "source": [
        "# If any value is NaN, the result is NaN\n",
        "df.sum(skipna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6kKAdR_6NxL"
      },
      "outputs": [],
      "source": [
        "df.sum(axis=1, skipna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGmlVqUr6NxL"
      },
      "source": [
        "**Indirect Statistics:** Finding *where* the max or min value is located (the index label)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAd6hOlR6NxL"
      },
      "outputs": [],
      "source": [
        "# Returns the index of the maximum value\n",
        "df.idxmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJlIqNEN6NxL"
      },
      "outputs": [],
      "source": [
        "# Returns the index of the minimum value\n",
        "df.idxmin()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUqS6UfO6NxL"
      },
      "source": [
        "**Accumulations:** Computing cumulative sums."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CH45T-Y6NxL"
      },
      "outputs": [],
      "source": [
        "df.cumsum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnOKImiW6NxL"
      },
      "source": [
        "**The `.describe()` method:** This is your best friend for a quick snapshot of numeric data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIQCEGQ26NxM"
      },
      "outputs": [],
      "source": [
        "# Provides count, mean, std, min, quartiles, and max\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRx8QQbj6NxM"
      },
      "source": [
        "**Categorical/Non-Numeric Data:** `.describe()` behaves differently for string data, showing counts and uniqueness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMH2TPfF6NxM"
      },
      "outputs": [],
      "source": [
        "obj = pd.Series([\"c\", \"a\", \"d\", \"a\", \"b\", \"b\", \"c\", \"c\"])\n",
        "\n",
        "# For object data, we get count, unique, top (most frequent), and freq\n",
        "obj.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQq9pdp36NxM"
      },
      "outputs": [],
      "source": [
        "# Get unique values\n",
        "obj.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVnc3gnz6NxM"
      },
      "outputs": [],
      "source": [
        "# Get frequency counts of each unique value\n",
        "obj.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07BalX146NxM"
      },
      "source": [
        "**Exercise:** Sort value counts in ascending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVVtLtzz6NxM"
      },
      "outputs": [],
      "source": [
        "obj.value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAi-cmbM6NxM"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLku5d576NxN"
      },
      "source": [
        "# Section 2: The Cleanup: Missing Data, Duplicates & Outliers\n",
        "\n",
        "**Goal:** Real-world data is dirty. In this section, we will learn techniques to identify and fix common quality issues.\n",
        "\n",
        "We will cover:\n",
        "1. **Missing Data:** Detecting nulls and deciding whether to drop them or fill them.\n",
        "2. **Duplicates:** Finding and removing repeated records.\n",
        "3. **Outliers:** identifying extreme values using boolean indexing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4gKa_Jh6NxN"
      },
      "source": [
        "## Part 1: Handling Missing Data\n",
        "\n",
        "Missing data is often represented as `NaN` (Not a Number) or `None`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDouAFES6NxN"
      },
      "source": [
        "> **ðŸ“Š Visual Illustration Available**: See ![missing_data](https://github.com/englikhong/6m-data-1.8-eda-basic/blob/main/visualisations/03_missing_data.png?raw=1) for visualizations of missing data patterns, heatmaps, and comparison of different strategies for handling missing values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ6AEXTs6NxN"
      },
      "outputs": [],
      "source": [
        "float_data = pd.Series([1.2, -3.5, np.nan, 0])\n",
        "\n",
        "float_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHsKgBsr6NxN"
      },
      "outputs": [],
      "source": [
        "# .isna() returns a boolean mask (True if missing, False if present)\n",
        "float_data.isna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsdmZiKT6NxN"
      },
      "source": [
        "The built-in Python `None` value is also treated as NA in pandas object arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucr9_Mq-6NxN"
      },
      "outputs": [],
      "source": [
        "string_data = pd.Series([\"aardvark\", np.nan, None, \"avocado\"])\n",
        "\n",
        "string_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOOR6_aZ6NxO"
      },
      "outputs": [],
      "source": [
        "string_data.isna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ExRTqML6NxO"
      },
      "source": [
        "### Strategy 1: Dropping Missing Data (`dropna`)\n",
        "The simplest strategy is to just remove the rows or columns that contain missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2NTHxng6NxO"
      },
      "outputs": [],
      "source": [
        "data = pd.Series([1, np.nan, 3.5, np.nan, 7])\n",
        "\n",
        "# Removes all NaN values from the Series\n",
        "data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6agRLYG6NxO"
      },
      "outputs": [],
      "source": [
        "# This is equivalent to boolean filtering:\n",
        "data[data.notna()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUjd07HD6NxO"
      },
      "source": [
        "With DataFrames, `dropna` by default drops **any row** containing **any missing value**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuGCKwur6NxO"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame([[1., 6.5, 3.], [1., np.nan, np.nan],\n",
        "                     [np.nan, np.nan, np.nan], [np.nan, 6.5, 3.]])\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Lxe2fxv6NxO"
      },
      "outputs": [],
      "source": [
        "# Drops rows 1, 2, and 3 because they have at least one NaN\n",
        "data.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zJEzWgH6NxO"
      },
      "source": [
        "We can control this behavior. `how='all'` only drops rows where **all** values are NaN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPZwSiJQ6NxP"
      },
      "outputs": [],
      "source": [
        "data.dropna(how=\"all\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lJ_yuOb6NxP"
      },
      "source": [
        "To drop **columns** instead of rows, pass `axis=1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gLo7Ca36NxP"
      },
      "outputs": [],
      "source": [
        "# Let's add a column of all NaNs first\n",
        "data[4] = np.nan\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yLGrJ9l6NxP"
      },
      "outputs": [],
      "source": [
        "# Drops the column '4' because it is all NaNs\n",
        "data.dropna(axis=\"columns\", how=\"all\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXX4ZBFS6NxP"
      },
      "source": [
        "We can also set a **threshold**: keep only rows containing at least `n` valid observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyXYGyXy6NxP"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(np.random.standard_normal((7, 3)))\n",
        "# Set some missing values\n",
        "df.iloc[:4, 1] = np.nan\n",
        "df.iloc[:2, 2] = np.nan\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DkB9AFY6NxP"
      },
      "outputs": [],
      "source": [
        "df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0TA3BVZ6NxP"
      },
      "outputs": [],
      "source": [
        "# Keep rows that have at least 2 non-NaN values\n",
        "df.dropna(thresh=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb--F4ij6NxQ"
      },
      "source": [
        "### Strategy 2: Filling Missing Data (`fillna`)\n",
        "Instead of losing data, we can fill the holes with a constant or a calculated value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQf7it8Z6NxQ"
      },
      "outputs": [],
      "source": [
        "# Replace all NaNs with 0\n",
        "df.fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86LhV7-L6NxQ"
      },
      "source": [
        "You can specify different fill values for each column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31BDXGi66NxQ"
      },
      "outputs": [],
      "source": [
        "# Fill column 1 with 0.5, and column 2 with 0\n",
        "df.fillna({1: 0.5, 2: 0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMTsgEvD6NxQ"
      },
      "source": [
        "**Forward/Backward Fill:** Useful for time-series data, where you propagate the last valid observation forward or backward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIOUXvdC6NxQ"
      },
      "outputs": [],
      "source": [
        "# Propagate next valid value backward to fill gaps\n",
        "df.fillna(method=\"bfill\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wp4X5bs6NxQ"
      },
      "outputs": [],
      "source": [
        "# Same, but limit how many rows are filled consecutively\n",
        "df.fillna(method=\"bfill\", limit=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnNGJSAD6NxR"
      },
      "source": [
        "**Imputation:** Filling with the mean or median is a very common technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QomZSLgj6NxR"
      },
      "outputs": [],
      "source": [
        "data = pd.Series([1., np.nan, 3.5, np.nan, 7])\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EgCdAbk6NxR"
      },
      "outputs": [],
      "source": [
        "# Fill with the mean of the available data\n",
        "data.fillna(data.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hftI79Hd6NxR"
      },
      "source": [
        "### Exercise: Handling Missing Data\n",
        "Practice using these methods on the dataframe below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFJsU5a86NxR"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(np.random.standard_normal((6, 3)))\n",
        "\n",
        "df.iloc[[2,4,5], 1] = np.nan\n",
        "df.iloc[4:, 2] = np.nan\n",
        "df.iloc[3:5, 0] = np.nan\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qkwy2uMp6NxR"
      },
      "source": [
        "> **Tasks:**\n",
        "> 1. Remove rows with *any* missing values.\n",
        "> 2. Remove rows with *all* missing values.\n",
        "> 3. Fill missing values with forward fill.\n",
        "> 4. Fill missing values with mean of the column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZoaMq5_6NxR"
      },
      "source": [
        "## Part 2: Handling Duplicates\n",
        "\n",
        "Duplicate rows can skew analysis and models. We typically identify them and remove them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qid44tB66NxR"
      },
      "source": [
        "> **ðŸ“Š Visual Illustration Available**: See ![duplicates](https://github.com/englikhong/6m-data-1.8-eda-basic/blob/main/visualisations/06_duplicates.png?raw=1) for visualizations showing how duplicates affect datasets and the impact of different `keep` parameter options.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvOYIUtj6NxS"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame({\"k1\": [\"one\", \"two\"] * 3 + [\"two\"],\n",
        "                     \"k2\": [1, 1, 2, 3, 3, 4, 4]})\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKbhkqO46NxS"
      },
      "source": [
        "`duplicated()` returns a boolean Series indicating whether each row has been seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-5SqFrW6NxS"
      },
      "outputs": [],
      "source": [
        "data.duplicated()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWckUktp6NxS"
      },
      "source": [
        "`drop_duplicates()` creates a new DataFrame with the duplicates removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQCDNjBB6NxS"
      },
      "outputs": [],
      "source": [
        "data.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hpb3UCW6NxS"
      },
      "source": [
        "**Subset:** Sometimes we only care about duplicates in specific columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGRFyr436NxS"
      },
      "outputs": [],
      "source": [
        "data[\"v1\"] = range(7)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Buin9O96NxS"
      },
      "outputs": [],
      "source": [
        "# Drop duplicates considering only column 'k1'\n",
        "data.drop_duplicates(subset=[\"k1\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7xMJVxf6NxS"
      },
      "source": [
        "**Keep:** By default, it keeps the first occurrence. We can keep the last one instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX66QBTR6NxS"
      },
      "outputs": [],
      "source": [
        "data.drop_duplicates(subset=[\"k1\", \"k2\"], keep=\"last\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7_ysMgb6NxT"
      },
      "source": [
        "## Part 3: Handling Outliers\n",
        "\n",
        "Outliers are extreme values that deviate significantly from the rest of the data. We can filter them using boolean indexing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzchKGpZ6NxT"
      },
      "source": [
        "> **ðŸ“Š Visual Illustration Available**: See ![outliers](https://github.com/englikhong/6m-data-1.8-eda-basic/blob/main/visualisations/04_outliers.png?raw=1) for visualizations of outlier detection methods (IQR, Z-score) and comparison of different handling strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQZ3j-nr6NxT"
      },
      "outputs": [],
      "source": [
        "# Create a dataset with normal distribution\n",
        "data = pd.DataFrame(np.random.standard_normal((1000, 4)))\n",
        "\n",
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0djmeHa6NxT"
      },
      "source": [
        "**Detection:** Let's find values exceeding 3 in absolute value (Standard Deviation > 3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHVDKLI96NxT"
      },
      "outputs": [],
      "source": [
        "col = data[2]\n",
        "\n",
        "# Boolean indexing to find rows where absolute value > 3\n",
        "col[col.abs() > 3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seKWKajr6NxT"
      },
      "source": [
        "To find **any row** that has an outlier in **any column**, we use `.any(axis=1)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8OdnNTM6NxT"
      },
      "outputs": [],
      "source": [
        "# 1. data.abs() > 3 returns a boolean DataFrame\n",
        "# 2. .any(axis=\"columns\") checks if any value in the row is True\n",
        "data[(data.abs() > 3).any(axis=\"columns\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9LiUpFs6NxT"
      },
      "source": [
        "**Capping:** Instead of removing outliers, we can cap them at a threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6q9CB2x6NxT"
      },
      "outputs": [],
      "source": [
        "# Set values > 3 to 3, and < -3 to -3, preserving the sign\n",
        "data[data.abs() > 3] = np.sign(data) * 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "me5x_Cco6NxU"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjvsr6636NxU"
      },
      "source": [
        "**Removal:** Or we can just drop the rows with outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsmymAwo6NxU"
      },
      "outputs": [],
      "source": [
        "# Keep rows where ALL columns are within the threshold ( < 3)\n",
        "data[(data.abs() < 3).all(axis=\"columns\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQJz2VTE6NxU"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6w7hONI6NxU"
      },
      "source": [
        "# Section 3: Refinement & Transformation\n",
        "\n",
        "**Goal:** Now that the data is clean, we need to transform it into the right format for analysis.\n",
        "\n",
        "We will cover:\n",
        "1.  **Mapping & Replacing:** Changing values based on a dictionary logic.\n",
        "2.  **String Manipulation:** Cleaning text data using the `.str` accessor.\n",
        "3.  **Exporting:** Saving your hard work to a file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVHf6VKO6NxU"
      },
      "source": [
        "## Part 1: Transforming Data (Mapping)\n",
        "\n",
        "Sometimes we need to add new columns based on existing ones. `map()` is perfect for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzzlPemF6NxU"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame({\"food\": [\"bacon\", \"pulled pork\", \"bacon\", \"pastrami\",\n",
        "                                \"corned beef\", \"bacon\", \"pastrami\", \"honey ham\",\n",
        "                                \"nova lox\"],\n",
        "                         \"ounces\": [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVHTIBdP6NxU"
      },
      "source": [
        "**Scenario:** We want to add a column indicating the animal source of each food."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1baV_zYs6NxU"
      },
      "outputs": [],
      "source": [
        "meat_to_animal = {\n",
        "    \"bacon\": \"pig\",\n",
        "    \"pulled pork\": \"pig\",\n",
        "    \"pastrami\": \"cow\",\n",
        "    \"corned beef\": \"cow\",\n",
        "    \"honey ham\": \"pig\",\n",
        "    \"nova lox\": \"salmon\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxhcxwTa6NxV"
      },
      "outputs": [],
      "source": [
        "# .map() looks up the value in the 'food' column in our dictionary\n",
        "data[\"animal\"] = data[\"food\"].map(meat_to_animal)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWaRz6ky6NxV"
      },
      "source": [
        "You can also pass a function to `map()` for custom logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01kDP4TU6NxV"
      },
      "outputs": [],
      "source": [
        "def get_animal(x):\n",
        "    return meat_to_animal[x]\n",
        "\n",
        "data[\"food\"].map(get_animal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mtv8bPR6NxV"
      },
      "source": [
        "**Replacing Values:** `replace` is a specialized version of map, great for fixing sentinel values (like -999 for missing data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFlxfvhk6NxV"
      },
      "outputs": [],
      "source": [
        "data = pd.Series([1, -999, 2, -999, -1000, 3])\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbxj8pkL6NxV"
      },
      "outputs": [],
      "source": [
        "# Replace -999 with NaN\n",
        "data.replace(-999, np.nan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pveE-Nk6NxV"
      },
      "outputs": [],
      "source": [
        "# Replace multiple values at once\n",
        "data.replace([-999, -1000], np.nan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UndCfuaz6NxV"
      },
      "outputs": [],
      "source": [
        "# Replace with different values (-999 -> NaN, -1000 -> 0)\n",
        "data.replace([-999, -1000], [np.nan, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1UOucJX6NxV"
      },
      "outputs": [],
      "source": [
        "# Using a dictionary for clarity\n",
        "data.replace({-999: np.nan, -1000: 0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP3dLlkQ6NxW"
      },
      "source": [
        "### Exercise: Transforming Data\n",
        "\n",
        "Try replacing values in a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tcg4fh2q6NxW"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(np.random.standard_normal((6, 3)))\n",
        "\n",
        "df.iloc[2:, 1] = -999\n",
        "df.iloc[4:, 2] = 999\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os-zQgzS6NxW"
      },
      "source": [
        "> **Task:** Replace -999 with NaN and 999 with 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnodgDoD6NxW"
      },
      "source": [
        "## Part 2: String Manipulation\n",
        "\n",
        "Pandas has a special accessor `.str` that unlocks string methods for an entire Series at once. This handles missing values gracefully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc5FcStN6NxW"
      },
      "outputs": [],
      "source": [
        "data = {\"Dave\": \"dave@google.com\", \"Steve\": \"steve@gmail.com\",\n",
        "        \"Rob\": \"rob@gmail.com\", \"Wes\": np.nan}\n",
        "\n",
        "data = pd.Series(data)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U1lzHa_6NxW"
      },
      "outputs": [],
      "source": [
        "# Check if 'gmail' exists in each string\n",
        "data.str.contains(\"gmail\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRhXTQa36NxW"
      },
      "source": [
        "Note on Data Types: Pandas has a specialized `StringDType` (`string`) vs the generic `object` type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtOAXYJ16NxW"
      },
      "outputs": [],
      "source": [
        "data_as_string = data.astype('string')\n",
        "\n",
        "data_as_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q10xIO7Q6NxW"
      },
      "outputs": [],
      "source": [
        "data_as_string.str.contains(\"gmail\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSHRFhtG6NxX"
      },
      "source": [
        "**Slicing:** We can treat the column like a Python string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AvrCjyP6NxX"
      },
      "outputs": [],
      "source": [
        "# Get the first 5 characters\n",
        "data_as_string.str[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoAB0uJe6NxX"
      },
      "source": [
        "**Regex:** Regular expressions allow for complex pattern matching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7OGvnBS6NxX"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hHafzH86NxX"
      },
      "outputs": [],
      "source": [
        "# Pattern to identify email parts: (username) @ (domain) . (suffix)\n",
        "pattern = r\"([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})\"\n",
        "\n",
        "data.str.findall(pattern, flags=re.IGNORECASE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I7ptdbP6NxX"
      },
      "source": [
        "**Retrieving elements:** We can chain `.str` calls to get specific parts of the regex match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0g0o7V76NxX"
      },
      "outputs": [],
      "source": [
        "matches = data.str.findall(pattern, flags=re.IGNORECASE).str[0]\n",
        "\n",
        "matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubMyyDiU6NxX"
      },
      "outputs": [],
      "source": [
        "# Get index 1 of the tuple (the domain name)\n",
        "matches.str.get(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMus78nQ6NxX"
      },
      "source": [
        "The `extract` method is very powerfulâ€”it creates a new DataFrame with columns for each captured regex group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RwQHM556NxY"
      },
      "outputs": [],
      "source": [
        "data.str.extract(pattern, flags=re.IGNORECASE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKdFCCv_6NxY"
      },
      "source": [
        "### Exercise: Strings\n",
        "\n",
        "> 1. Get the 1st group of the regex email pattern (the username).\n",
        "> 2. Convert data into titlecase (e.g. \"Dave@Google.Com\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzqHjJw66NxY"
      },
      "source": [
        "## Part 3: Writing Data (Exporting)\n",
        "\n",
        "Once your data is cleaned, you need to save it. `to_csv` is the most common method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDLPHPcE6NxY"
      },
      "outputs": [],
      "source": [
        "result.to_csv(\"../data/out.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDR2TnBz6NxY"
      },
      "outputs": [],
      "source": [
        "!cat ../data/out.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0VPsAN56NxY"
      },
      "source": [
        "**Tip:** You usually want `index=False` to avoid saving the row numbers as a separate column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "720-zVAR6NxY"
      },
      "outputs": [],
      "source": [
        "result.to_csv(\"../data/out.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5BSKzZY6NxY"
      },
      "outputs": [],
      "source": [
        "!cat ../data/out.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTA-KIo26NxZ"
      },
      "source": [
        "**Excel Export:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hwUefko6NxZ"
      },
      "outputs": [],
      "source": [
        "# Option 1: Using ExcelWriter (good for multiple sheets)\n",
        "writer = pd.ExcelWriter('../data/out.xlsx')\n",
        "\n",
        "frame.to_excel(writer, 'Sheet1')\n",
        "\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vIc4LhZ6NxZ"
      },
      "outputs": [],
      "source": [
        "# Option 2: Direct export\n",
        "frame.to_excel('../data/out.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XXJdEyt6NxZ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGdRYG8p6NxZ"
      },
      "source": [
        "## Optional Topics\n",
        "\n",
        "If time permits, explore these advanced transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DbmxJ386NxZ"
      },
      "source": [
        "### Renaming Axis Indexes\n",
        "Changing row/column labels using mapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYLq-mft6NxZ"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(np.arange(12).reshape((3, 4)),\n",
        "                    index=['Ohio', 'Colorado', 'New York'],\n",
        "                    columns=['one', 'two', 'three', 'four'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oyyywi4v6Nxa"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53Zja_Jk6Nxa"
      },
      "source": [
        "Using `.map()` on the index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8hzWlNz6Nxa"
      },
      "outputs": [],
      "source": [
        "def transform(x):\n",
        "    return x[:4].upper()\n",
        "\n",
        "data.index.map(transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb2BzSv16Nxa"
      },
      "outputs": [],
      "source": [
        "# Assign back to modify in-place\n",
        "data.index = data.index.map(transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNoDes526Nxa"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_r8NZbs6Nxa"
      },
      "source": [
        "Using `.rename()` (returns a copy by default):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-1AMqhJ6Nxa"
      },
      "outputs": [],
      "source": [
        "data.rename(index=str.title, columns=str.upper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHWzcqsd6Nxb"
      },
      "outputs": [],
      "source": [
        "data.rename(index={\"OHIO\": \"INDIANA\"}, columns={\"three\": 3})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN5I_LE_6Nxb"
      },
      "source": [
        "### Permutation and Random Sampling\n",
        "Reordering or selecting random subsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiTcDba96Nxb"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(np.arange(5 * 7).reshape((5, 7)))\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNHFZm9I6Nxb"
      },
      "outputs": [],
      "source": [
        "# Create a random order of indices\n",
        "sampler = np.random.permutation(5)\n",
        "\n",
        "sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7Z3Zsnd6Nxc"
      },
      "outputs": [],
      "source": [
        "# Reorder rows\n",
        "df.iloc[sampler]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4v_pY4d6Nxc"
      },
      "outputs": [],
      "source": [
        "# Alternative: use .take()\n",
        "df.take(sampler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9iQZfSW6Nxc"
      },
      "source": [
        "Permuting columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRVxJXEv6Nxc"
      },
      "outputs": [],
      "source": [
        "column_sampler = np.random.permutation(df.shape[1])\n",
        "\n",
        "column_sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOLkaRNI6Nxc"
      },
      "outputs": [],
      "source": [
        "df.take(column_sampler, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWNgbQhM6Nxc"
      },
      "source": [
        "**Random Sample:** Getting a random subset without manually creating a sampler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9GNuM7w6Nxc"
      },
      "outputs": [],
      "source": [
        "df.sample(n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ke9BuS16Nxc"
      },
      "outputs": [],
      "source": [
        "choices = pd.Series([5, 7, -1, 6, 4])\n",
        "\n",
        "# Sample with replacement (items can be picked more than once)\n",
        "choices.sample(n=10, replace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhzZ_n336Nxc"
      },
      "source": [
        "> **Task:** Sample `df` using the parameter `frac` (percentage) instead of `n` (count)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbIINipk6Nxc"
      },
      "source": [
        "### Categorical Data\n",
        "Converting string columns to `category` dtype saves memory and speeds up operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5ZzKY3E6Nxc"
      },
      "source": [
        "> **ðŸ“Š Visual Illustration Available**: See ![categorical_binning](../visualisations/05_categorical_binning.png` for visualizations comparing `pd.cut()` vs `pd.qcut()`, categorical operations, and dummy variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCufuhle6Nxd"
      },
      "outputs": [],
      "source": [
        "values = pd.Series([\"apple\", \"orange\", \"apple\", \"apple\"] * 2)\n",
        "\n",
        "values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57WXDCRK6Nxd"
      },
      "outputs": [],
      "source": [
        "values.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br4X8FPj6Nxd"
      },
      "outputs": [],
      "source": [
        "values.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs1OoK3q6Nxd"
      },
      "source": [
        "**Under the hood:** Categoricals are stored as integers referencing a dictionary of values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyVAmfNf6Nxd"
      },
      "outputs": [],
      "source": [
        "values = pd.Series([0, 1, 0, 0] * 2)\n",
        "\n",
        "dim = pd.Series([\"apple\", \"orange\"])\n",
        "\n",
        "values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlfjbuEH6Nxd"
      },
      "outputs": [],
      "source": [
        "dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnLijtzQ6Nxd"
      },
      "outputs": [],
      "source": [
        "dim.take(values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYr5hYAQ6Nxd"
      },
      "source": [
        "**Using Pandas `category` type:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9HKHgdP6Nxe"
      },
      "outputs": [],
      "source": [
        "fruits = [\"apple\", \"orange\", \"apple\", \"apple\"] * 2\n",
        "N = len(fruits)\n",
        "\n",
        "# to ensure reproducibility\n",
        "rng = np.random.default_rng(seed=12345)\n",
        "\n",
        "df = pd.DataFrame({\"fruit\": fruits,\n",
        "                   \"basket_id\": np.arange(N),\n",
        "                   \"count\": rng.integers(3, 15, size=N),\n",
        "                   \"weight\": rng.uniform(0, 4, size=N)},\n",
        "                  columns=[\"basket_id\", \"fruit\", \"count\", \"weight\"])\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0V7kjX36Nxe"
      },
      "outputs": [],
      "source": [
        "# Convert object column to category\n",
        "fruit_cat = df['fruit'].astype('category')\n",
        "\n",
        "fruit_cat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ0jE0TE6Nxe"
      },
      "source": [
        "The values for `fruit_cat` are now an instance of `pandas.Categorical`, which you can access via the `.array` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8mHkmLO6Nxe"
      },
      "outputs": [],
      "source": [
        "c = fruit_cat.array\n",
        "\n",
        "type(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbph-cA26Nxe"
      },
      "outputs": [],
      "source": [
        "c.categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ksR7yK_6Nxe"
      },
      "outputs": [],
      "source": [
        "c.codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7pxMdzl6Nxf"
      },
      "outputs": [],
      "source": [
        "dict(enumerate(c.categories))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jVIMYiE6Nxf"
      },
      "outputs": [],
      "source": [
        "# Assign back to the dataframe\n",
        "df['fruit'] = df['fruit'].astype('category')\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oS1iDvr6Nxf"
      },
      "source": [
        "**Creating from Codes:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00oXFzmd6Nxf"
      },
      "outputs": [],
      "source": [
        "categories = ['foo', 'bar', 'baz']\n",
        "codes = [0, 1, 2, 0, 0, 1]\n",
        "\n",
        "my_cats = pd.Categorical.from_codes(codes, categories)\n",
        "\n",
        "my_cats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtLsj98V6Nxf"
      },
      "source": [
        "**Ordered Categoricals:** Useful for Likert scales or sizes (Small < Medium < Large)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2qSm9cI6Nxf"
      },
      "outputs": [],
      "source": [
        "ordered_cats = pd.Categorical.from_codes(codes, categories, ordered=True)\n",
        "\n",
        "ordered_cats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKMUbTsJ6Nxf"
      },
      "source": [
        "**Binning Data (`pd.cut`):** Converting continuous data (e.g., Age) into categorical bins (e.g., Age Groups)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSnirS1w6Nxf"
      },
      "outputs": [],
      "source": [
        "ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-bB_DHh6Nxf"
      },
      "outputs": [],
      "source": [
        "# Define bin edges\n",
        "bins = [18, 25, 35, 60, 100]\n",
        "\n",
        "# Cut the data\n",
        "age_cat = pd.cut(ages, bins)\n",
        "\n",
        "age_cat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ2c1O7B6Nxg"
      },
      "outputs": [],
      "source": [
        "age_cat.codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIO0sdtm6Nxg"
      },
      "outputs": [],
      "source": [
        "age_cat.categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1ESHrQy6Nxg"
      },
      "outputs": [],
      "source": [
        "age_cat.categories[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn4eROwD6Nxg"
      },
      "outputs": [],
      "source": [
        "pd.value_counts(age_cat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WLd8SY86Nxg"
      },
      "source": [
        "**Binning Parameters:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74uD3fJx6Nxg"
      },
      "outputs": [],
      "source": [
        "# right=False makes the left side closed [inclusive, exclusive)\n",
        "pd.cut(ages, bins, right=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du4F-Wpn6Nxg"
      },
      "outputs": [],
      "source": [
        "# Adding custom labels\n",
        "group_names = [\"Youth\", \"YoungAdult\", \"MiddleAged\", \"Senior\"]\n",
        "\n",
        "pd.cut(ages, bins, labels=group_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RzCgl_I6Nxg"
      },
      "source": [
        "If you pass an integer number of bins to `pd.cut`, it will compute equal-length bins based on the min/max of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwhdzJO86Nxg"
      },
      "outputs": [],
      "source": [
        "data = np.random.uniform(size=20)\n",
        "\n",
        "pd.cut(data, 4, precision=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpwcqcGZ6Nxg"
      },
      "source": [
        "> **Task:** Cut `ages` into 5 bins instead of 4. Set the labels to `['Youth', 'YoungAdult', 'MiddleAged', 'Senior', 'Elderly']`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLX4aScv6Nxg"
      },
      "source": [
        "**Categorical Methods (.cat):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhoardCZ6Nxg"
      },
      "outputs": [],
      "source": [
        "s = pd.Series(['a', 'b', 'c', 'd'] * 2)\n",
        "cat_s = s.astype('category')\n",
        "\n",
        "cat_s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wptdcos06Nxh"
      },
      "outputs": [],
      "source": [
        "cat_s.cat.codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGB2V_BM6Nxh"
      },
      "outputs": [],
      "source": [
        "cat_s.cat.categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBA3r29d6Nxh"
      },
      "source": [
        "Modifying categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEdeZgjQ6Nxh"
      },
      "outputs": [],
      "source": [
        "actual_categories = ['a', 'b', 'c', 'd', 'e']\n",
        "cat_s2 = cat_s.cat.set_categories(actual_categories)\n",
        "\n",
        "cat_s2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vpBxdmP6Nxh"
      },
      "outputs": [],
      "source": [
        "cat_s.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1l_NMvE06Nxh"
      },
      "outputs": [],
      "source": [
        "cat_s2.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmgRzV3C6Nxh"
      },
      "outputs": [],
      "source": [
        "cat_s2.cat.remove_unused_categories()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phv5BzqF6Nxh"
      },
      "source": [
        "**Computing Indicator / Dummy Variables:** Converting categorical variables into binary columns (One-Hot Encoding) for machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxNElzBI6Nxh"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({\"key\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"b\"],\n",
        "                   \"data1\": range(6)})\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUZ3M_0s6Nxh"
      },
      "outputs": [],
      "source": [
        "pd.get_dummies(df[\"key\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVMa1xxw6Nxi"
      },
      "outputs": [],
      "source": [
        "dummies = pd.get_dummies(df[\"key\"], prefix=\"key\")\n",
        "\n",
        "dummies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzepywCh6Nxi"
      },
      "source": [
        "Recipe: Combining `get_dummies` with `cut`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYA0O_o76Nxi"
      },
      "outputs": [],
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "values = np.random.uniform(size=10)\n",
        "\n",
        "values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRgnKKq56Nxi"
      },
      "outputs": [],
      "source": [
        "bins = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
        "\n",
        "pd.get_dummies(pd.cut(values, bins))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW_xassZ6Nxi"
      },
      "source": [
        "> **Task:** Cut values into 4 bins and create dummy variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0pVT4I16Nxi"
      },
      "source": [
        "### Databases\n",
        "\n",
        "Connecting to a database using `sqlalchemy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rnc9CGZ6Nxi"
      },
      "outputs": [],
      "source": [
        "import sqlalchemy as sqla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncV3mrvv6Nxi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "parent_dir = os.path.abspath(os.path.pardir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVJudSta6Nxi"
      },
      "outputs": [],
      "source": [
        "# Create connection string\n",
        "engine = sqla.create_engine(f'duckdb:///{parent_dir}/data/unit-1-4.db')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wfc5ne8U6Nxi"
      },
      "outputs": [],
      "source": [
        "# Read entire table\n",
        "df = pd.read_sql('resale_flat_prices_2017', engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjEnWcAI6Nxi"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkD59JHG6Nxj"
      },
      "outputs": [],
      "source": [
        "# Execute raw SQL query\n",
        "df = pd.read_sql(\"SELECT * FROM resale_flat_prices_2017\", engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JS5RYsU6Nxj"
      },
      "outputs": [],
      "source": [
        "engine.table_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK9ColfY6Nxj"
      },
      "source": [
        "### Binary files (Pickle)\n",
        "\n",
        "`pickle` is Python's native serialization format. Good for short-term storage, but not for sharing data between different languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGxKIgNL6Nxj"
      },
      "outputs": [],
      "source": [
        "result.to_pickle('../data/out.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_ZIjoDj6Nxj"
      },
      "outputs": [],
      "source": [
        "data = pd.read_pickle('../data/out.pkl')\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA13gtZK6Nxj"
      },
      "source": [
        "> **Task:** Write a filtered DataFrame to a new database table.\n",
        "> 1. Filter for flats in \"YISHUN\".\n",
        "> 2. Write to table `yishun_flat_prices_2017`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOjV8_uo6Nxj"
      },
      "outputs": [],
      "source": [
        "df_yishun = df[df.town == \"YISHUN\"]\n",
        "\n",
        "df_yishun.to_sql(\"yishun_flat_prices_2017\", engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnB29kEv6Nxk"
      },
      "source": [
        "> **Final Challenge:**\n",
        "> 1. Read only flats from `BISHAN` to a new dataframe.\n",
        "> 2. Write the dataframe to a new database table `bishan_flat_prices_2017`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pds",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}